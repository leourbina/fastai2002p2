# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/clean/09_learner.ipynb.

# %% auto 0
__all__ = ['torch_device', 'device', 'Callback', 'run_cbs', 'CancelFitException', 'CancelBatchException', 'CancelEpochException',
           'to_cpu', 'MetricsCB', 'DeviceCB', 'with_cbs', 'Learner', 'ProgressCB', 'MomentumLearner', 'LRFinderCB']

# %% ../nbs/clean/09_learner.ipynb 1
import math
import torch
from torch import nn, tensor
import matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from operator import attrgetter
from functools import partial
from copy import copy
from tqdm.auto import trange, tqdm

from torch import optim
import torch.nn.functional as F

from .conv import *
from fastprogress import progress_bar, master_bar

torch_device = "mps" if torch.backends.mps.is_available() else "gpu" if torch.cuda.is_available() else "cpu"
device = torch.device(torch_device)

# %% ../nbs/clean/09_learner.ipynb 13
class Callback: 
    order = 0
    
    def before_fit(self, learn): pass
    def after_fit(self, learn): pass
    def before_epoch(self, learn): pass
    def after_epoch(self, learn): pass
    def before_batch(self, learn): pass
    def after_batch(self, learn): pass

# %% ../nbs/clean/09_learner.ipynb 14
def run_cbs(cbs, method_name, learn=None):
    for cb in sorted(cbs, key=attrgetter('order')):
        method = getattr(cb, method_name, None)
        if method is not None: method(learn)

# %% ../nbs/clean/09_learner.ipynb 15
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/clean/09_learner.ipynb 28
from torcheval.metrics import MulticlassAccuracy, Mean

# %% ../nbs/clean/09_learner.ipynb 32
def to_cpu(x):
    if isinstance(x, Mapping): return {k: to_cpu(v) for k, v in x.items()}
    if isinstance(x, list): return [to_cpu(o) for o in x]
    if isinstance(x, tuple): return tuple(to_cpu(list(x)))
    return x.detach().cpu()

# %% ../nbs/clean/09_learner.ipynb 33
class MetricsCB(Callback):
    def __init__(self, *ms, **metrics):
        for o in ms: 
            print("registering metric", type(o).__name__)
            metrics[type(o).__name__] = o
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics['loss'] = self.loss = Mean()
        
    def _log(self, d): print(d)
    def before_fit(self, learn): learn.metrics = self
    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]
    
    def after_epoch(self, learn):
        log = {k: f'{v.compute():.3f}' for k, v in self.all_metrics.items()}    
        log['epoch'] = learn.epoch
        log['train'] = 'train' if learn.model.training else 'eval'
        self._log(log)
    
    def after_batch(self, learn):
        x, y, *_ = to_cpu(learn.batch)
        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)
        self.loss.update(to_cpu(learn.loss), weight=len(x))

# %% ../nbs/clean/09_learner.ipynb 34
class DeviceCB(Callback):
    def __init__(self, device=device): self.device = device
    def before_fit(self, learn):
        if hasattr(learn.model, 'to'): learn.model.to(self.device)
    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)

# %% ../nbs/clean/09_learner.ipynb 38
class with_cbs:
    def __init__(self, name): self.name = name
    def __call__(self, f):
        def _f(o, *args, **kwargs):
            try:
                o._callback(f'before_{self.name}')
                f(o, *args, **kwargs)
                o._callback(f'after_{self.name}')
            except globals()[f'Cancel{self.name.title()}Exception']: pass
            finally: o._callback(f'cleanup_{self.name}')
        return _f

class Learner():
    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):
        cbs = fc.L(cbs)
        fc.store_attr()

    def fit(self, n_epochs=1, train=True, valid=True, lr=None, cbs=None):
        cbs = fc.L(cbs)
        for cb in cbs: self.cbs.append(cb) # add extra cbs
        try:
            self.n_epochs = n_epochs
            self.epochs = range(n_epochs)
            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)
            self._fit(train, valid)
        finally:
            for cb in cbs: self.cbs.remove(cb) # cleanup extra cbs

    @with_cbs('fit')
    def _fit(self, train, valid):
        for self.epoch_idx, self.epoch in enumerate(self.epochs):
            if train: 
                self.one_epoch(True)
            if valid:
                with torch.no_grad():
                    self.one_epoch(False)
   
    def one_epoch(self, train):
        self.model.train(train)
        self.dl = self.dls.train if train else self.dls.valid
        self._one_epoch(train)

    @with_cbs('epoch')
    def _one_epoch(self, train):
        for self.iter, self.batch in enumerate(self.dl):
            self.one_batch()
            
    @with_cbs('batch')    
    def one_batch(self):
        self.predict()
        self.get_loss()
        if self.model.training:
            self.backward()
            self.step()
            self.zero_grad()  

    # these are defined here to allow subclassing to customize behavior
    def predict(self): self.preds = self.model(self.batch[0])
    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])
    def backward(self): self.loss.backward()
    def step(self): self.opt.step()
    def zero_grad(self): self.opt.zero_grad()

    def _callback(self, method_name): run_cbs(self.cbs, method_name, self)
    
    @property
    def training(self):
        return self.model.training

# %% ../nbs/clean/09_learner.ipynb 40
class ProgressCB(Callback):
    order = MetricsCB.order + 1
    def __init__(self, plot=False): self.plot = plot
    def before_fit(self, learn):
        self.first = True
        learn.epochs = self.mbar = master_bar(learn.epochs)
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.losses = []
        
    def _log(self, d):
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first=False
        self.mbar.write(list(d.values()), table=True)
        
    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl)
        
    def after_batch(self, learn):
        learn.dl.comment = f"{learn.loss:.3f}"
        if self.plot and hasattr(learn, 'metrics') and learn.model.training:
            self.losses.append(learn.loss.item())
            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])

# %% ../nbs/clean/09_learner.ipynb 45
class MomentumLearner(Learner):
    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):
        self.mom = mom
        super().__init__(model, dls, loss_func, lr, cbs, opt_func)
    
    def zero_grad(self):
        with torch.no_grad():
            for p in self.model.parameters():
                # Instead of zeroing out the gradients, we just keep a residue
                p.grad *= self.mom 

# %% ../nbs/clean/09_learner.ipynb 49
class LRFinderCB(Callback):
    def __init__(self, lr_mult=1.3): fc.store_attr()
    
    def before_fit(self, learn):
        self.lrs, self.losses = [], []
        self.min = math.inf
        
    def after_batch(self, learn):
        if not learn.model.training: raise CancelEpochException() # Only find LR during training
        self.lrs.append(learn.opt.param_groups[0]['lr'])
        loss = to_cpu(learn.loss)
        self.losses.append(loss)
        if loss < self.min: self.min = loss
        if loss > self.min*3: CancelFitException()
        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult
        

# %% ../nbs/clean/09_learner.ipynb 51
from torch.optim.lr_scheduler import ExponentialLR

# %% ../nbs/clean/09_learner.ipynb 52
class LRFinderCB(Callback):
    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()
    
    def before_fit(self, learn):
        self.scheduler = ExponentialLR(learn.opt, self.gamma)
        self.lrs, self.losses = [], []
        self.min = math.inf
        
    def after_batch(self, learn):
        if not learn.model.training: raise CancelEpochException()
        self.lrs.append(learn.opt.param_groups[0]['lr'])
        loss = to_cpu(learn.loss)
        self.losses.append(loss)
        if loss < self.min: self.min = loss
        if math.isnan(loss) or loss > self.max_mult*self.min: 
            raise CancelFitException()
        self.scheduler.step()
        
    def cleanup_fit(self, learn):
        plt.plot(self.lrs, self.losses)
        plt.xscale('log')
