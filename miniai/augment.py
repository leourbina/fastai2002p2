# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/clean/14_augment.ipynb.

# %% auto 0
__all__ = ['summary']

# %% ../nbs/clean/14_augment.ipynb 2
import torch
import random
import fastcore.all as fc

from torch import nn
from torch.nn import init

from .datasets import *
from .conv import *
from .learner import *
from .activations import * 
from .init import *
from .sgd import *
from .resnet import *

# %% ../nbs/clean/14_augment.ipynb 13
def _flops(x, h, w):
    if x.dim() <= 3: return x.numel()
    if x.dim() == 4: return x.numel()*h*w

@fc.patch
def summary(self:Learner):
    res = '|Module|Input|Output|Num params|MFlops|\n|--|--|--|--|--|\n'
    totp, totf = 0, 0
    @hook
    def _f(hook, model, inp, out):
        nonlocal res, totp, totf
        num_params = sum(o.numel() for o in model.parameters())
        totp += num_params
        *_, h, w = out.shape
        flops = sum(_flops(o, h, w) for o in model.parameters())/1e6
        totf += flops
        res += f'|{type(model).__name__}|{tuple(inp[0].shape)}|{tuple(out.shape)}|{num_params}|{flops:.1f}|\n'
    
    with Hooks(self.model, h=_f) as h: self.fit(1, lr=1, cbs=SingleBatchCB())
    print(f"Total params: {totp} - Mflops: {totf:.1f}")
    
    if fc.IN_NOTEBOOK:
        from IPython.display import Markdown
        return Markdown(res)
    else: print(res)        
